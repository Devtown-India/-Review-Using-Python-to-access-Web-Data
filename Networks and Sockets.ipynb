{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transport Control Protocol (TCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Built on top of IP(Internet Protocol)<br>\n",
    "- Assumes IP might lose some data stored and retransmits data if it seems to be lost<br>\n",
    "- Handles \"flow control\" using a transmit window<br>\n",
    "- Provides a nice reliable pipe<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCP Connections / Sockets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In computer networking, an Internet socket or network socket is an endpoint of a bidirectional inter-process communication flow across an Internet Protocol-based computer network, such as the Internet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCP Port Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A port is an application-specific or process-specific software communication endpoint<br>\n",
    "- It allows multiple networked applications to coexist on the same server<br>\n",
    "- There is a list of well-known TCP port numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common TCP Ports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Telnet (23) - Login \n",
    "- SSH (22) - Secure Login\n",
    "- HTTP (80)\n",
    "- HTTPS (443) - Secure\n",
    "- SMTP (25)(Mail)\n",
    "- IMAP (143/220/2993) - Mail Retrieval\n",
    "- POP (109/110) - Mail Retrieval\n",
    "- DNS (53) - Domain Name\n",
    "- FTP (21) - File Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sockets in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has built-in support for TCP Sockets. Socket programming is a way of connecting two nodes on a network to communicate with each other. One socket(node) listens on a particular port at an IP, while other socket reaches out to the other to form a connection. Server forms the listener socket while client reaches out to the server.<br><br>\n",
    "They are the real backbones behind web browsing. In simpler terms there is a server and a client. \n",
    "Socket programming is started by importing the socket library and making a simple socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we made a socket instance and passed it two parameters. The first parameter is <b> AF_INET </b> and the second one is <b> SOCK_STREAM. </b> AF_INET refers to the address family ipv4. The SOCK_STREAM means connection oriented TCP protocol. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TCP (and python) gives us a reliable socket, what do we want to do with the socket? What problem do we want to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application-level network protocols can also be accessed using high-level access provided by Python libraries. These protocols are HTTP, FTP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP - HyperText Transfer Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dominant Application Layer Protocol on the Internet<br>\n",
    "- Invented for the Web - to Retrieve HTML, Images, Documents, etc<br>\n",
    "- Extended to be data in addition to documents - RSS, Web Services, etc.. Basic Concept - Make a connection - Request a document - Retrieve the Document - Close the Connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HyperText Transfer Protocol is the set of rules to allow browsers to retreive web documents from servers over the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### What is a Protocol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of rules that all parties follow so we can predict each other's behaviour and not bump into eaachother. The Internet Protocol is designed to implement a uniform system of addresses on all of the Internet-connected computers everywhere and to make it possible for packets to travel from one end of the Internet to the other. A program like the web browser should be able to connect to a host anywhere without ever knowing which maze of network devices each packet is traversing on its journey. There are various categories of internet protocols. These protocols are created to serve the needs of different types of data communication between different computers in the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From The Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time the user clicks on an anchor tag with an href= value to switch to a new page, the browser makes a connection to the web server and issues a \"GET\" request - to GET the content of the page at the specified URL. The server returns the HTML document to the browser which formats and displays the document to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internet Standards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The standards for all the Internet protocols are developed by an organization<br>\n",
    "- Internet Engineering Task Force (IETF)\n",
    "- Standards are called \"RFCs\" - \"Request for Comments\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An HTTP Request in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Fri, 07 Aug 2020 09:22:05 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already s\n",
      "ick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode())\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, <b> data.pr4e.org </b> is Host and <b> 80 </b> is Port<br>\n",
    "<b> mysock.recv() </b>actually reads the data <br>\n",
    "<b> href=\"(+)\" </b> is the regular expression that extract the URL <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the program makes a connection to port 80 on the server www.py4e.com. Since our program is playing the role of the “web browser”, the HTTP protocol says we must send the GET command followed by a blank line. \\r\\n signifies an EOL (end of line), so \\r\\n\\r\\n signifies nothing between two EOL sequences. That is the equivalent of a blank line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we send that blank line, we write a loop that receives data in 512-character chunks from the socket and prints the data out until there is no more data to read (i.e., the recv() returns an empty string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the server sends us the headers, it adds a blank line to indicate the end of the headers, and then sends the actual data of the file romeo.txt.<br><br>\n",
    "This example shows how to make a low-level network connection with sockets. Sockets can be used to communicate with a web server or with a mail server or many other kinds of servers. All that is needed is to find the document which describes the protocol and write the code to send and receive the data according to the protocol.<br><br>\n",
    "However, since the protocol that we use most commonly is the HTTP web protocol, Python has a special library specifically designed to support the HTTP protocol for the retrieval of documents and data over the web.<br> <br>\n",
    "One of the requirements for using the HTTP protocol is the need to send and receive data as bytes objects, instead of strings. In the preceding example, the encode() and decode() methods convert strings into bytes objects and back again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Characters and Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Simple Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each character is represented by a nuber between 0 and 256 stored in 8 bits of memory <br>\n",
    "- We refer to \"8 bits of memory as a \"byte\" of memory - (i.e. my disk drive contains 3 Terabytes of memory) <br>\n",
    "- The ord() function tells us the numeric value of a simple ASCII character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"G\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Byte Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the wide range of characters, we represent characters with more than one byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UTF-16 - Fixed length - Two bytes <br>\n",
    "- UTF-32 - Fixed length - Four bytes <br>\n",
    "- UTF-8  - 1-4 bytes <br>\n",
    "    * Upwards compatible with ASCII <br>\n",
    "    * Automatic detection between ASCII and UTF-8 <br>\n",
    "    * UTF-8 is recommended practice for encoding data to be exchanged between systems <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = u'abc'\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = b'abc'\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Python 3, all strings internally are UNICODE<br>\n",
    "- Working with string variables in Python programs and reading data from files usually \"just works\"<br>\n",
    "- When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Strings to Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we talk to an external resource like a network socket we sends bytes, so we need to encode Python 3 strings into a given character encoding.<br>\n",
    "- nWhen we read data from an external resource, we must decode it based on the character set so it is properly represented in Python 3 as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using urllib in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since HTTP is so common, we have a library that does all the socket work for us and makes web pages look like a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request,urllib.parse,urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> import urllib.request,urllib.parse,urllib.error </b> has data structure most similar to file handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request,urllib.parse,urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0)+1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When a program or script pretends to be a browser and retrives web pages, looks at those web pages, extracts information, and then looks at more web pages.<br>\n",
    "- Before scraping a web site you should check that the web site allows scraping.<br>\n",
    "- Search engine scrape web pages - we call this \"spidering the web\" or \"web crawling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pull data - particularly social data - who links to who? <br>\n",
    "- Get your own data back out of some system that has no \"export capability\"<br>\n",
    "- Monitor a site for new information<br>\n",
    "- Spider the web to make a database for a search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is some controversy about web page scraping and some sites are a bit snippy about it.<br>\n",
    "- Republishing copyrighted information is not allowed.<br>\n",
    "- Violating terms of services is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL: http://py4e-data.dr-chuck.net/comments_42.html\n",
      "Enter count: 50\n",
      "Enter position: 18\n",
      "Retrieving: http://py4e-data.dr-chuck.net/comments_42.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "link = input('Enter URL: ')\n",
    "cont = int(input('Enter count: '))\n",
    "line = int(input('Enter position: '))\n",
    "\n",
    "print('Retrieving: %s' % link)\n",
    "for i in range(0, cont):\n",
    "    html = urllib.request.urlopen(link, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    cn = 0\n",
    "    ps = 0\n",
    "    for tag in tags:\n",
    "        ps += 1\n",
    "        if ps == line:\n",
    "            print('Retrieving: %s' % str(tag.get('href', None)))\n",
    "            link = str(tag.get('href', None))\n",
    "            ps = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Start at: http://py4e-data.dr-chuck.net/known_by_Rogan.html\n",
    "\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Hint: The first character of the name of the last page that you will load is: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL: http://py4e-data.dr-chuck.net/known_by_Rogan.html\n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Rogan.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Nicodemus.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Luic.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Macy.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Dolci.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Alber.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Allen.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Grzegorz.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "#SSL Certification Error Handle\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#Data Collection\n",
    "link = input('Enter URL: ')\n",
    "cont = int(input('Enter count: '))\n",
    "line = int(input('Enter position: '))\n",
    "\n",
    "\n",
    "\n",
    "print('Retrieving: %s' % link)\n",
    "for i in range(0, cont):\n",
    "    html = urllib.request.urlopen(link, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    tags = soup('a')\n",
    "    cn = 0\n",
    "    ps = 0\n",
    "    for tag in tags:\n",
    "        ps += 1\n",
    "        if ps == line:\n",
    "            print('Retrieving: %s' % str(tag.get('href', None)))\n",
    "            link = str(tag.get('href', None))\n",
    "            ps = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Numbers from HTML using BeautifulSoup <br>\n",
    "The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/comments_704348.html\n",
      "2713\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "tags = soup('span')\n",
    "sum = 0\n",
    "for tag in tags:\n",
    "    sum = sum+int(tag.contents[0])\n",
    "print (sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
